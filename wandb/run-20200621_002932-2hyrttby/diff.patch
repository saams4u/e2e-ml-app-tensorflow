diff --git a/README.md b/README.md
index 1bb2fbe..b5ec151 100644
--- a/README.md
+++ b/README.md
@@ -147,7 +147,7 @@ text-classification/
 |   ├── predict.py                        - prediction script
 |   ├── streamlit.py                      - streamlit app
 |   ├── train.py                          - training script
-|   └── utils.py                          - load embeddings
+|   └── utils.py                          - load embeddings and utilities
 ├── wandb/                              - wandb experiment runs
 ├── .dockerignore                       - files to ignore on docker
 ├── .gitignore                          - files to ignore on git
diff --git a/requirements.txt b/requirements.txt
index 61b3234..742ba2f 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -5,7 +5,7 @@ pandas==1.0.3
 pytest==5.4.1
 sklearn==0.0
 streamlit==0.60.0
-tensorflow-cpu==2.1.0
+tensorflow-cpu==2.2.0
 tqdm==4.43.0
 uvicorn==0.11.3
 wandb==0.8.34
\ No newline at end of file
diff --git a/text_classification/data.py b/text_classification/data.py
index 581e7e6..b0de0fb 100644
--- a/text_classification/data.py
+++ b/text_classification/data.py
@@ -33,6 +33,22 @@ def load_data(url, data_size):
     return X, y
 
 
+def preprocess_texts(texts):
+    preprocessed_texts = []
+    for text in texts:
+        # remove items text in () ex. (Reuters)
+        # may want to refine to only remove if at end of text
+        text = re.sub(r'\([^)]*\)', '', text)
+
+        # spacing
+        text = re.sub(r"([.,!?])", r" \1 ", text)
+        text = re.sub(' +', ' ', text)  # remove multiple spaces
+        text = text.strip()
+
+        preprocessed_texts.append(text)
+    return preprocessed_texts
+
+
 def train_val_test_split(X, y, val_size, test_size, shuffle):
     X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=test_size, stratify=y, shuffle=shuffle)
diff --git a/text_classification/models.py b/text_classification/models.py
index b451d5e..43e0c92 100644
--- a/text_classification/models.py
+++ b/text_classification/models.py
@@ -1,3 +1,5 @@
+import tensorflow as tf
+from tensorflow import keras
 from tensorflow.keras.layers import Concatenate
 from tensorflow.keras.layers import Conv1D
 from tensorflow.keras.layers import Dense
@@ -5,11 +7,14 @@ from tensorflow.keras.layers import Dropout
 from tensorflow.keras.layers import Embedding
 from tensorflow.keras.layers import GlobalMaxPool1D
 from tensorflow.keras.layers import Input
+from tensorflow.keras.losses import SparseCategoricalCrossentropy
+from tensorflow.keras.metrics import SparseCategoricalAccuracy
 from tensorflow.keras.models import Model
+from tensorflow.keras.optimizers import Adam
 from tensorflow.keras.utils import plot_model
 
 
-class TextCNN(Model):
+class TextCNN(keras.Model):
     def __init__(self, vocab_size, embedding_dim, filter_sizes, num_filters,
                  hidden_dim, dropout_p, num_classes, freeze_embeddings=False):
         super(TextCNN, self).__init__(name="cnn")
@@ -62,6 +67,32 @@ class TextCNN(Model):
 
         return logits
 
+    def compile(self, learning_rate):
+        super(TextCNN, self).compile()
+        self.optimizer = Adam(lr=learning_rate)
+        self.loss_fn = SparseCategoricalCrossentropy()
+        self.accuracy_fn = SparseCategoricalAccuracy()
+
+    def train_step(self, batch):
+        X, y, _ =  batch
+
+        with tf.GradientTape() as tape:
+            y_pred = self.call(X, training=True)
+            loss = self.loss_fn(y, y_pred)
+            accuracy = self.accuracy_fn(y, y_pred)
+
+        grads = tape.gradient(loss, self.trainable_weights)
+        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
+
+        return {'loss':loss, 'accuracy': accuracy}
+
+    def test_step(self, batch):
+        X, y =  batch
+        y_pred = self.call(X)
+        loss = self.loss_fn(y, y_pred)
+        accuracy = self.accuracy_fn(y, y_pred)
+        return {'loss':loss, 'accuracy': accuracy}
+
     def summary(self, input_shape):
         x_in = Input(shape=input_shape, name='X')
         summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)
diff --git a/text_classification/train.py b/text_classification/train.py
index 63dfc63..4517a5c 100644
--- a/text_classification/train.py
+++ b/text_classification/train.py
@@ -20,9 +20,6 @@ from tensorflow.keras.callbacks import Callback
 from tensorflow.keras.callbacks import EarlyStopping
 from tensorflow.keras.callbacks import ModelCheckpoint
 from tensorflow.keras.callbacks import ReduceLROnPlateau
-from tensorflow.keras.losses import SparseCategoricalCrossentropy
-from tensorflow.keras.metrics import SparseCategoricalAccuracy
-from tensorflow.keras.optimizers import Adam
 from tensorflow.keras.preprocessing.text import Tokenizer
 
 from text_classification import config
@@ -156,6 +153,13 @@ if __name__ == '__main__':
         "Raw data:\n"
         f"  {X[0]} {y[0]}")
 
+    # Preprocess (filtering is done later via tokenizer)
+    original_X = X
+    X = data.preprocess_texts(texts=X)
+    config.logger.info(
+        "Preprocessed data:\n"
+        f"  {original_X[0]} → {X[0]}")
+
     # Split data
     X_train, X_val, X_test, y_train, y_val, y_test = data.train_val_test_split(
         X=X, y=y, val_size=args.val_size, test_size=args.test_size, shuffle=args.shuffle)
@@ -167,7 +171,8 @@ if __name__ == '__main__':
 
     # Tokenizer
     X_tokenizer = Tokenizer(
-        filters=args.filters, lower=args.lower, char_level=args.char_level, oov_token='<UNK>')
+        filters=args.filters, lower=args.lower,
+        char_level=args.char_level, oov_token='<UNK>')
     X_tokenizer.fit_on_texts(X_train)
     vocab_size = len(X_tokenizer.word_index) + 1  # +1 for padding token
     config.logger.info(f"vocab_size: {vocab_size}")
@@ -269,9 +274,7 @@ if __name__ == '__main__':
                  WandbCallback()]
 
     # Compile
-    model.compile(optimizer=Adam(lr=args.learning_rate),
-                  loss=SparseCategoricalCrossentropy(),
-                  metrics=[SparseCategoricalAccuracy()])
+    model.compile(learning_rate=args.learning_rate)
 
     # Training
     training_history = model.fit(
@@ -279,16 +282,16 @@ if __name__ == '__main__':
         callbacks=callbacks, shuffle=False, class_weight=class_weights, verbose=1)
 
     # Evaluation
-    test_history = model.evaluate(x=testing_generator, verbose=1)
+    test_history = model.evaluate(x=testing_generator, return_dict=True)
     y_pred = model.predict(x=testing_generator, verbose=1)
     y_pred = np.argmax(y_pred, axis=1)
-    test_loss, test_acc = test_history[0:2]
     config.logger.info(
         "Test performance:\n"
-        f"  test_loss: {test_loss:.2f}, test_acc: {test_acc:.1f}")
+        f"  test_loss: {test_history['loss']:.2f}\n"
+        f"  test_acc: {test_history['accuracy']:.1f}")
     wandb.log({
-        "test_loss": test_loss,
-        "test_accuracy": test_acc})
+        "test_loss": test_history['loss'],
+        "test_accuracy": test_history['accuracy']})
 
     # Per-class performance analysis
     performance = get_performance(y_pred, y_test, classes)
